name: lbg App CI/CD

on:
  push:
    branches: [main, qwert]
  pull_request:
    branches: [main]

env:
  ACR_LOGIN_SERVER: ${{ secrets.ACR_LOGIN_SERVER }}
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  NODE_VERSION: '18'
  TERRAFORM_VERSION: '1.6.6'  # CHANGED: Downgraded to stable version
  HELM_VERSION: '3.14.0'

jobs:
  Build-terraform-infrastructure:
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Azure Login with Service Principal
        run: |
          az login --service-principal \
            -u ${{ secrets.AZURE_CLIENT_ID }} \
            -p ${{ secrets.AZURE_CLIENT_SECRET }} \
            --tenant ${{ secrets.AZURE_TENANT_ID }}
          az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Create Resource Group if not exists
        run: |
          if ! az group show --name kamalakar-demo-dev &>/dev/null; then
            echo "Creating resource group: kamalakar-demo-dev"
            az group create \
              --name kamalakar-demo-dev \
              --location "East US" \
              --tags "Environment=dev" "Project=lbg-app"
          else
            echo "Resource group already exists"
          fi

      - name: Create Storage Account for Terraform Backend
        run: |
          # Check if storage account exists
          if ! az storage account show --name tfstatelkamalj2k0212 --resource-group kamalakar-demo-dev &>/dev/null; then
            echo "Creating storage account: tfstatelkamalj2k0212"
            az storage account create \
              --name tfstatelkamalj2k0212 \
              --resource-group kamalakar-demo-dev \
              --location "East US" \
              --sku Standard_LRS \
              --kind StorageV2
            
            echo "Waiting for storage account to be provisioned..."
            sleep 30
            
            echo "Creating blob container"
            az storage container create \
              --name lbg-02-12-1994 \
              --account-name tfstatelkamalj2k0212 \
              --auth-mode login
            echo "Storage account and container created successfully"
          else
            echo "Storage account already exists"
          fi

      - name: Terraform Init
        run: |
          cd infrastructure
          terraform init \
            -backend-config="resource_group_name=kamalakar-demo-dev" \
            -backend-config="storage_account_name=tfstatelkamalj2k0212" \
            -backend-config="container_name=lbg-02-12-1994" \
            -backend-config="key=terraform.lbg-02-12-1994"
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

      - name: Import Existing Resources
        run: |
          cd infrastructure
          
          # Import resource group if it exists
          if az group show --name kamalakar-demo-dev &>/dev/null; then
            echo "Importing existing resource group..."
            terraform import module.resource_group.azurerm_resource_group.main /subscriptions/${{ secrets.AZURE_SUBSCRIPTION_ID }}/resourceGroups/kamalakar-demo-dev || echo "Resource group may already be imported"
          fi
          
          # Refresh state after import
          terraform refresh -lock-timeout=10m
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

      - name: Terraform Validate
        run: |
          cd infrastructure
          terraform validate
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

      - name: Terraform Format
        run: |
          cd infrastructure
          terraform fmt -recursive

      - name: Terraform Plan
        run: |
          cd infrastructure
          terraform plan \
            -var="environment=dev" \
            -var="resource_group_name=kamalakar-demo-dev" \
            -var="location=East US" \
            -var="acr_name=acrkamaldemodev" \
            -var="aks_cluster_name=kamal-lgb-aks-cluster" \
            -var="vnet_name=kamal-vnet-dev-lbg-app" \
            -var="public_ip_name=kamal-lbg-dev-healthcare-app" \
            -var="vm_size=Standard_D2s_v3" \
            -var="kubernetes_version=1.33.3" \
            -var="min_count=1" \
            -var="max_count=5" \
            -out=tfplan
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

      - name: Terraform Apply with Crash Recovery
        if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/qwert'
        run: |
          cd infrastructure
          echo "Applying Terraform plan on branch: ${{ github.ref }}"
          
          # Apply with increased timeout and retry logic for crashes
          max_attempts=3
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            echo "Attempt $attempt of $max_attempts"
            if terraform apply -auto-approve -lock-timeout=10m tfplan; then
              echo "Terraform apply completed successfully"
              break
            else
              echo "Terraform apply failed or crashed"
              if [ $attempt -eq $max_attempts ]; then
                echo "All attempts failed. Exiting..."
                exit 1
              fi
              echo "Waiting 30 seconds before retry..."
              sleep 30
              attempt=$((attempt + 1))
            fi
          done
        env:
          ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}

  build-and-push:
    runs-on: ubuntu-latest
    needs: Build-terraform-infrastructure
    strategy:
      matrix:
        service: [patient-service, appointment-service]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies for ${{ matrix.service }}
        run: |
          cd ${{ matrix.service }}
          npm install

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Azure Container Registry with Service Principal
        run: |
          az login --service-principal \
            -u ${{ secrets.AZURE_CLIENT_ID }} \
            -p ${{ secrets.AZURE_CLIENT_SECRET }} \
            --tenant ${{ secrets.AZURE_TENANT_ID }}
          az acr login --name acrkamaldemodev

      - name: Build and Push Docker image for ${{ matrix.service }}
        run: |
          cd ${{ matrix.service }}
          
          docker build \
            --tag ${{ secrets.ACR_LOGIN_SERVER }}/${{ matrix.service }}:${{ github.sha }} \
            --tag ${{ secrets.ACR_LOGIN_SERVER }}/${{ matrix.service }}:latest \
            --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
            --build-arg VCS_REF=${{ github.sha }} \
            --build-arg VERSION=1.0.0 \
            .
          
          docker push ${{ secrets.ACR_LOGIN_SERVER }}/${{ matrix.service }}:${{ github.sha }}
          docker push ${{ secrets.ACR_LOGIN_SERVER }}/${{ matrix.service }}:latest
          
          echo "Successfully built and pushed ${{ matrix.service }}:${{ github.sha }}"

  trivy-scan:
    runs-on: ubuntu-latest
    needs: build-and-push
    strategy:
      matrix:
        service: [patient-service, appointment-service]
    steps:
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@0.24.0
        with:
          image-ref: '${{ secrets.ACR_LOGIN_SERVER }}/${{ matrix.service }}:${{ github.sha }}'
          format: 'table'
          severity: 'CRITICAL,HIGH'
          exit-code: 0

      - name: Upload Trivy results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results-${{ matrix.service }}
          path: |
            trivy-results-*.json
            trivy-results-*.sarif
          retention-days: 30

  security-scan:
    runs-on: ubuntu-latest
    needs: build-and-push
    steps:
      - uses: actions/checkout@v4

      - name: Run npm audit
        run: |
          echo "Running npm audit for patient-service..."
          cd patient-service && npm audit --audit-level high || echo "npm audit completed with warnings"
          echo "Running npm audit for appointment-service..."
          cd ../appointment-service && npm audit --audit-level high || echo "npm audit completed with warnings"

  deploy-to-dev:
    runs-on: ubuntu-latest
    needs: [Build-terraform-infrastructure, trivy-scan, security-scan]
    environment: dev
    steps:
      - uses: actions/checkout@v4

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh --version v${{ env.HELM_VERSION }}
          helm version

      - name: Configure Azure credentials with Service Principal
        run: |
          az login --service-principal \
            -u ${{ secrets.AZURE_CLIENT_ID }} \
            -p ${{ secrets.AZURE_CLIENT_SECRET }} \
            --tenant ${{ secrets.AZURE_TENANT_ID }}
          az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AKS credentials
        run: |
          az aks get-credentials \
            --resource-group kamalakar-demo-dev \
            --name kamal-lgb-aks-cluster-dev \
            --overwrite-existing

      - name: Setup Kubernetes context
        run: |
          kubectl config current-context
          kubectl get nodes

      - name: Install Nginx Ingress Controller
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.service.type=LoadBalancer \
            --set controller.ingressClassResource.name=nginx \
            --set controller.ingressClassResource.controllerValue="k8s.io/ingress-nginx" \
            --wait \
            --timeout 5m

      - name: Wait for LoadBalancer IP
        run: |
          echo "Waiting for LoadBalancer IP assignment..."
          timeout 300s bash -c 'until kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath="{.status.loadBalancer.ingress[0].ip}" | grep -q "."; do sleep 5; done'
          EXTERNAL_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          echo "External IP: $EXTERNAL_IP"
          echo "external_ip=$EXTERNAL_IP" >> $GITHUB_ENV

      - name: Handle existing namespace
        run: |
          if kubectl get namespace lbg-ns >/dev/null 2>&1; then
            echo "Namespace lbg-ns already exists"
            if helm list -n lbg-ns -q | grep -q "lbg-app"; then
              echo "Helm release 'lbg-app' exists, will perform upgrade"
            else
              echo "No Helm release found. Deleting namespace for clean installation..."
              kubectl delete namespace lbg-ns
              if kubectl wait --for=delete namespace/lbg-ns --timeout=120s 2>/dev/null; then
                echo "Namespace deleted successfully"
              else
                echo "Namespace deletion completed"
              fi
              sleep 10
            fi
          else
            echo "Namespace lbg-ns does not exist, it will be created"
          fi

      - name: Create ACR pull secret
        run: |
          kubectl create namespace lbg-ns --dry-run=client -o yaml | kubectl apply -f -
          kubectl create secret docker-registry acr-secret \
            --namespace lbg-ns \
            --docker-server=${{ secrets.ACR_LOGIN_SERVER }} \
            --docker-username=${{ secrets.AZURE_CLIENT_ID }} \
            --docker-password=${{ secrets.AZURE_CLIENT_SECRET }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy to AKS using Helm
        run: |
          cd kubernetes/helm
          helm template lbg-app . \
            --namespace lbg-ns \
            -f values-dev.yml \
            --set patientService.tag=${{ github.sha }} \
            --set appointmentService.tag=${{ github.sha }} \
            --set image.registry=${{ secrets.ACR_LOGIN_SERVER }}
          helm upgrade --install lbg-app . \
            --namespace lbg-ns \
            --wait \
            --timeout 15m \
            -f values-dev.yml \
            --set patientService.tag=${{ github.sha }} \
            --set appointmentService.tag=${{ github.sha }} \
            --set image.registry=${{ secrets.ACR_LOGIN_SERVER }} \
            --atomic

      - name: Verify deployment
        run: |
          echo "=== Deployment Verification ==="
          kubectl get all -n lbg-ns
          echo ""
          kubectl get ingress -n lbg-ns
          echo ""
          kubectl get hpa -n lbg-ns 2>/dev/null || echo "HPA not configured"
          echo ""
          kubectl get pvc -n lbg-ns 2>/dev/null || echo "No PVCs found"

      - name: Get Browser Access URLs
        run: |
          EXTERNAL_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          echo "üéâ DEPLOYMENT SUCCESSFUL!"
          echo ""
          echo "üì± BROWSER ACCESS INFORMATION:"
          echo "================================"
          echo "External IP: $EXTERNAL_IP"
          echo ""
          echo "üìç Add this to your /etc/hosts file:"
          echo "$EXTERNAL_IP lbg-app.dev.local"
          echo ""
          echo "üåê Access your services in browser:"
          echo "   Patient Service:    http://lbg-app.dev.local/patients"
          echo "   Appointment Service: http://lbg-app.dev.local/appointments" 
          echo "   Health Check:       http://lbg-app.dev.local/health"
          echo ""
          echo "üí° Quick test command:"
          echo "   echo '$EXTERNAL_IP lbg-app.dev.local' | sudo tee -a /etc/hosts"

      - name: Run comprehensive health checks
        run: |
          echo "=== Running Health Checks ==="
          echo "Waiting for pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=patient-service -n lbg-ns --timeout=300s
          kubectl wait --for=condition=ready pod -l app=appointment-service -n lbg-ns --timeout=300s
          echo "Testing internal service health..."
          kubectl exec -n lbg-ns deployment/patient-service -- curl -s -f http://localhost:3000/health && echo "‚úÖ Patient service internal health: OK" || echo "‚ùå Patient service internal health: FAILED"
          kubectl exec -n lbg-ns deployment/appointment-service -- curl -s -f http://localhost:3001/health && echo "‚úÖ Appointment service internal health: OK" || echo "‚ùå Appointment service internal health: FAILED"
          echo "Testing service communication..."
          kubectl exec -n lbg-ns deployment/patient-service -- curl -s -f http://appointment-service:3001/health && echo "‚úÖ Patient -> Appointment communication: OK" || echo "‚ùå Patient -> Appointment communication: FAILED"
          kubectl exec -n lbg-ns deployment/appointment-service -- curl -s -f http://patient-service:3000/health && echo "‚úÖ Appointment -> Patient communication: OK" || echo "‚ùå Appointment -> Patient communication: FAILED"
          echo "Testing through ingress..."
          EXTERNAL_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          if [ -n "$EXTERNAL_IP" ]; then
            curl -s -f http://$EXTERNAL_IP/health && echo "‚úÖ Ingress health check: OK" || echo "‚ùå Ingress health check: FAILED"
          fi

  cleanup:
    runs-on: ubuntu-latest
    if: always()
    needs: [deploy-to-dev]
    steps:
      - name: Cleanup Docker images
        run: |
          docker system prune -f
